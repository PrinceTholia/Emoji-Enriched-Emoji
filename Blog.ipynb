{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multimodal Speech Emotion Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title & Author"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speech Emotion Recognition: Fusing Audio and Text for Multimodal Emotion Analysis\n",
    "\n",
    "~ Prince Tholia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emotion recognition from speech is a fascinating challenge with very less work done even when the field of AI and Machine Learning has developed so far by now. I chose this topic because it exemplifies the power of multimodal data analysis-leveraging both what is said (text) and how it is said (audio)-to infer human emotion more accurately than either modality alone. This has real-world relevance in areas like virtual assistants, call centers, voice transcriptions for audio impaired beings, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection to Multimodal Learning: A Brief Perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multimodal learning brings together different types of information—like images, audio, and text—to help models better understand and interpret data. Thanks to recent progress in deep learning, we can now combine these modalities more effectively, which has led to major improvements in areas such as video captioning, visual question answering, and emotion recognition.\n",
    "\n",
    "Speech Emotion Recognition (SER) is a great example of a multimodal task. By blending audio cues—like tone, pitch, and energy—with the actual words being spoken, we can make much more accurate predictions about someone’s emotional state. Modern approaches, such as ECAPA-TDNN for processing audio and transformer-based models for analyzing text, highlight how powerful specialized deep learning models can be. When used together with smart fusion techniques, they can significantly boost performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Learning Journey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I explored multiple open-source datasets for emotion-labeled speech, including RAVDESS, CREMA-D, TESS, SAVEE, EMOVO, Urdu, and EmoDB. Each dataset provides audio samples annotated with emotions like happy, sad, angry, fear, disgust, surprise, and neutral.\n",
    "\n",
    "- Preprocessing involved:\n",
    "\n",
    "    - Audio: Loading files, normalizing, and visualizing waveforms and spectrograms.\n",
    "\n",
    "    - Text: Using automatic speech recognition (ASR) tools (e.g., Vosk) to transcribe audio into text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Audio: Extracted features such as MFCCs, zero-crossing rate, spectral centroid, RMS energy, and pitch. Data augmentation (noise, pitch shift, time stretch) was used to improve model robustness.\n",
    "\n",
    "- Text: Used transformer-based models (DistilRoBERTa fine-tuned for emotion classification) to analyze sentiment and emotion from transcribed text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Audio Model: Implemented a deep 1D CNN (inspired by ECAPA-TDNN) in PyTorch. Trained on extracted features, it achieved high accuracy (~95%) on test data.\n",
    "\n",
    "- Text Model: Used HuggingFace pipelines with pre-trained emotion classifiers for text sentiment analysis.\n",
    "\n",
    "- Fusion: Combined predictions from both modalities using a weighted average, allowing for flexible emphasis on either audio or text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experimentation & Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstrated the pipeline on sample audio files, showing:\n",
    "\n",
    "- Audio waveform and spectrogram.\n",
    "\n",
    "- Transcribed text.\n",
    "\n",
    "- Emotion probabilities from both modalities.\n",
    "\n",
    "- Fused emotion prediction and confidence.\n",
    "\n",
    "- Enhanced output with appropriate emoji for the detected emotion.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"output.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code & Demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Audio preprocessing and feature extraction\n",
    "\n",
    "import librosa\n",
    "y, sr = librosa.load('sample.wav', duration=2.5, offset=0.6)\n",
    "mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "\n",
    "# Plot waveform and spectrogram\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.waveshow(y, sr=sr)\n",
    "plt.title('Waveform')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Transcribe audio to text using Vosk\n",
    "\n",
    "from vosk import Model, KaldiRecognizer\n",
    "wf = wave.open('sample.wav', \"rb\")\n",
    "model = Model('vosk-model-small-en-us-0.15')\n",
    "rec = KaldiRecognizer(model, wf.getframerate())\n",
    "text = ''\n",
    "while True:\n",
    "    data = wf.readframes(4000)\n",
    "    if len(data) == 0:\n",
    "        break\n",
    "    if rec.AcceptWaveform(data):\n",
    "        part = json.loads(rec.Result())\n",
    "        text += part.get('text', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Text emotion analysis\n",
    "\n",
    "from transformers import pipeline\n",
    "emotion_classifier = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\", top_k=None)\n",
    "emotion_scores = emotion_classifier(text)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Audio emotion analysis (deep model)\n",
    "\n",
    "import torch\n",
    "model = AudioModel(input_shape=(2376,), num_classes=7)\n",
    "model.load_state_dict(torch.load('audio_model.pth'))\n",
    "features = extract_features(y, sr)\n",
    "features_tensor = torch.tensor(features, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "outputs = model(features_tensor)\n",
    "probabilities = torch.softmax(outputs, dim=1).cpu().numpy()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Fusion and visualization\n",
    "\n",
    "fused_emotions = {emo: 0.6 * text_probs.get(emo, 0) + 0.4 * audio_probs.get(emo, 0) for emo in all_emotions}\n",
    "plt.bar(fused_emotions.keys(), fused_emotions.values())\n",
    "plt.title('Fused Emotion Probabilities')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What surprised me?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The accuracy boost from combining modalities was significant, especially in ambiguous cases where either the text or the tone alone was insufficient.\n",
    "\n",
    "- Every person has different context for each emoji in their own mind, which makes it difficult to map emotions to emojis accurately for large crowd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scope for Improvement\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Integrate end-to-end multimodal models (e.g., transformers that jointly process audio and text).\n",
    "\n",
    "- Explore more sophisticated fusion techniques, such as attention-based weighting or late fusion with confidence calibration.\n",
    "\n",
    "- Use larger, more diverse datasets and experiment with cross-lingual emotion recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RAVDESS Dataset\n",
    "\n",
    "- CREMA-D Dataset\n",
    "\n",
    "- Vosk Speech Recognition Toolkit\n",
    "\n",
    "- ECAPA-TDNN Paper\n",
    "\n",
    "- DistilRoBERTa Emotion Model"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
